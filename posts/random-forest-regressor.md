---
title: "Tree-Based Regression Methods (Part II): Random Forests"
date: July 17, 2020
slug: random-forest-regressor
---

## Introduction and Recap

In the previous [post](/p/decision-tree-regressor), we introduced the concept
of *decision trees* for the purpose of regression in the context of supervised
learning.
Decision trees are very simple models, which are easy to understand and apply,
but which suffer from rather poor performance as they tend to be fairly biased
towards the training data.
Without deliberate measures to limit the complexity of the constructed trees,
we may end up with trees where each leaf contains exactly one training sample
in the extreme case.
Imposing limits on the tree depth, the minimum number of samples required in a
leaf node, or the minimum number of samples to split an internal node can all
help improve the generalization of trees.
The performance on unseen data ultimately remains rather poor unfortunately.

One common way to combat this effect is by considering *ensembles* of trees,
where each tree in the ensemble "votes" on the final
prediction.
*Random forests*, the topic of this post, are a popular method in this
category, which consider *randomized ensembles* constructed in a way that
avoids some issues that more naive ensemble methods suffer from.

The rest of the post is structured as follows.
We first explain how an existing random forest is used to perform prediction on
new samples.
We then briefly explain how random forests are constructed, before going
through a simple Python implementation that builds on the `Tree` class that
we wrote in the context of our decision tree regressor.

> The Python code we will be discussing below can be found under the following
> tag of the Github repository:
> https://github.com/nkoep/fundamental-ml/tree/v2-random-forest.

## Prediction via Ensembles of Trees

As alluded to before, random forests are conceivably simple to apply in
practice.
Given an ensemble of trained decision tree regressors, ensemble methods such
as random forests simply combine the individual predictions into a concensus
prediction of the entire ensemble.
In a classification task, this is a simple majority vote, i.e., the most
commonly predicted class among all trees wins, whereas in the context of
regression, the target predictions of each tree are averaged to form the final
prediction of the ensemble.
More concretely, consider a family of decision trees $\family =
\setpred{\function{f_i}{\R^\nfeat}{\R}}{i = 1, \ldots, \nest}$.
Given an unseen observation $\vmx \in \R^\nfeat$, the random forest regressor
returns
$$
  \yhat
  = \frac{1}{\nest} \sum_{i=1}^\nest f_i(\vmx).
$$
And that's all there is to it.
Since all trees are created independently from each other (as we will discuss
next), each tree's prediction contributes equally to the final prediction.
This is in stark contrast to some of the more advanced methods we'll be looking
at in future posts, where different members of the ensemble might have more
influence on the final result than others.

## Seeing the Random Forest for the Decision Trees

In this section, we will briefly go over the construction of random forests.
Since a random forest is just a collection of trees trained on independently
sampled subsets of the training set, the heavy lifting happens in the
construction of individual trees.

### Bootstrapping, Aggregating and Bagging

### Aside: Random Sampling of the Training Set

There is one section in the parameter description of scikit-learn's
`RandomForestRegressor` class that first might appear a little
counterintuitive.
When bootstrapping is used to construct the individual trees (the default
behavior), the number of samples drawn for each tree is controlled by the
`max_samples` parameter, which defaults to the total number of training
samples.
At first glance, this sounds like the samples are simply shuffled (which
wouldn't affect the resulting forest as the training set is unordered to begin
with).
The key is that observations are sampled uniformly at random from the training
set *with* replacement.
Intuitively, the probability of selecting each sample only once is vanishingly
small, so we can generally expect that a certain fraction of samples is chosen
multiple times.
The question now is how many unique samples are drawn on average.

To frame this question mathematically, let $S$ be a random subset of $[\nsamp]
\defeq \set{1, \ldots, \nsamp}$ generated by picking $\nsamp$ values uniformly
at random from $[\nsamp]$ with replacement.
Then $\card{S}$ is a discrete random variable supported on $[\nsamp]$.
In order to evaluate $\E\card{S}$, we'll use a common trick in probability
theory by expressing $\card{S}$ in terms of indicator functions.
In particular, denote by $\event_i$ the event that $i \in S$.
Then $|S| = \sum_{i=1}^\nsamp \ind{\event_i}$, where
$\ind{\event_i}$ is $1$ if $\event_i$ happens, and 0 otherwise.
By linearity of expectation, this yields
$$
  \E\card{S}
  = \sum_{i=1}^\nsamp \E\ind{\event_i}
  = \sum_{i=1}^\nsamp \P(\event_i)
  = \sum_{i=1}^\nsamp (1 - \P(\comp{\event_i})).
$$
It remains to estimate the probability of the complementary event
$\comp{\event_i}$, i.e., we never pick $i$ when drawing $\nsamp$ elements
from $[\nsamp]$.
Since each element of $[\nsamp]$ is equally likely, we have by independence of
individual draws that
$$
  \P(\comp{\event_i})
  = \parens{\frac{\nsamp - 1}{\nsamp}}^\nsamp.
$$
Overall, this yields
$$
  \E\card{S}
  = \nsamp\parens{1 - \parens{\frac{\nsamp - 1}{\nsamp}}^\nsamp}.
$$

Moreover, by [Hoeffding's
inquality](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality#General_case_of_bounded_random_variables)
the random variable $\card{S}$ concentrates sharply around its mean.
To see this, note that by the previous representation of $\card{S}$, we have
$$
  \card{S} - \E\card{S}
  = \sum_{i=1}^\nsamp (\ind{\event_i} - \E\ind{\event_i})
  = \sum_{i=1}^\nsamp \parens{\ind{\event_i} - 1 +
  \parens{\frac{\nsamp-1}{\nsamp}}^\nsamp}
  \eqdef \sum_{i=1}^\nsamp X_i.
$$
Clearly, we have that $\E X_i = 0$ and $\abs{X_i} \leq 1$ a.s. for all $i \in
[\nsamp]$.
Hence Hoeffding's inequality states that for $t > 0$,
$$
  \P(\abs{\card{S} - \E\card{S}} \geq t)
  = \P\parens{\abs{\sum_{i=1}^\nsamp X_i} \geq t}
  \leq 2 \exp\parens{-\frac{t^2}{2\nsamp}}.
$$

Note that with the limit representation $e^x = \lim_{n \to \infty} (1 +
x/n)^n$, for large $\nsamp$ we roughly have that
$$
  \E\card{S}
  \approx \nsamp (1 - e^{-1})
  \approx 0.63212 \nsamp.
$$
This means that by drawing $\nsamp$ samples uniformly at random from the
training set of size $\nsamp$ with replacement, on average we will use around
2/3 of the training set to fit each individual tree in the ensemble.

## Python Implementation of a Random Forest Regressor

### Speeding Up Our Decision Tree Regressor with Numba

## Closing Remarks
